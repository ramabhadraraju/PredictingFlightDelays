{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'month,dated,WorkDay,unique_carrier,origin_city_name,dest_city_name,dephrs,arrhrs,schduration,distance,delay',\n u'1,2013-01-04,WEEKEND,UA,Chicago,Dallas/Fort,12,14,150,802,0',\n u'1,2013-01-04,WEEKEND,UA,Denver,Dallas/Fort,10,13,116,641,0',\n u'1,2013-01-04,WEEKEND,UA,Dallas/Fort,Chicago,13,16,136,802,0',\n u'1,2013-01-04,WEEKEND,UA,Newark,Dallas/Fort,6,9,247,1372,0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# @hidden_cell\n",
    "# This function accesses a file in your Object Storage. The definition contains your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "def set_hadoop_config_with_credentials_fcbce0c99dd24dad845fde405c1a6547(name):\n",
    "    \"\"\"This function sets the Hadoop configuration so it is possible to\n",
    "    access data from Bluemix Object Storage V3 using Spark\"\"\"\n",
    "\n",
    "    prefix = 'fs.swift.service.' + name\n",
    "    hconf = sc._jsc.hadoopConfiguration()\n",
    "    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n",
    "    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n",
    "    hconf.set(prefix + '.tenant', 'dec963a04d9c4fe78ec0684a8e298a6f')\n",
    "    hconf.set(prefix + '.username', 'e3beadc2523f4bbba7c9e59d6f4cfe95')\n",
    "    hconf.set(prefix + '.password', 'D[t(E51_YwiZCIjm')\n",
    "    hconf.setInt(prefix + '.http.port', 8080)\n",
    "    hconf.set(prefix + '.region', 'dallas')\n",
    "    hconf.setBoolean(prefix + '.public', True)\n",
    "\n",
    "# you can choose any name\n",
    "name = 'keystone'\n",
    "set_hadoop_config_with_credentials_fcbce0c99dd24dad845fde405c1a6547(name)\n",
    "\n",
    "input_rdd = sc.textFile(\"swift://finalproject.\" + name + \"/Ontime2013dfw.csv\")\n",
    "input_rdd.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1,2013-01-04,WEEKEND,UA,Chicago,Dallas/Fort,12,14,150,802,0',\n u'1,2013-01-04,WEEKEND,UA,Denver,Dallas/Fort,10,13,116,641,0',\n u'1,2013-01-04,WEEKEND,UA,Dallas/Fort,Chicago,13,16,136,802,0',\n u'1,2013-01-04,WEEKEND,UA,Newark,Dallas/Fort,6,9,247,1372,0',\n u'1,2013-01-04,WEEKEND,UA,Dallas/Fort,Denver,16,17,128,641,0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataheader= input_rdd.first()\n",
    "dfw_data = input_rdd.filter(lambda x : x != dataheader)\n",
    "dfw_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(line):\n",
    "    fields = line.split(',')\n",
    "    month = float(fields[0])\n",
    "    workDay = fields[2]\n",
    "    unique_carrier = fields[3]\n",
    "    origin_city_name = fields[4]\n",
    "    dest_city_name = fields[5]\n",
    "    dephrs = float(fields[6])\n",
    "    arrhrs = float(fields[7])\n",
    "    schduration = float(fields[8])\n",
    "    distance = float(fields[9])\n",
    "    delay = float(fields[10])\n",
    "    cls = ''\n",
    "    if delay < 15:\n",
    "        label = '0'\n",
    "    else:\n",
    "        label = '1'\n",
    "    return  (month,unique_carrier,origin_city_name,dest_city_name,dephrs,arrhrs,schduration,distance,float(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfw_rdd = dfw_data.map(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, u'UA', u'Chicago', u'Dallas/Fort', 12.0, 14.0, 150.0, 802.0, 0.0),\n (1.0, u'UA', u'Denver', u'Dallas/Fort', 10.0, 13.0, 116.0, 641.0, 0.0),\n (1.0, u'UA', u'Dallas/Fort', u'Chicago', 13.0, 16.0, 136.0, 802.0, 0.0),\n (1.0, u'UA', u'Newark', u'Dallas/Fort', 6.0, 9.0, 247.0, 1372.0, 0.0),\n (1.0, u'UA', u'Dallas/Fort', u'Denver', 16.0, 17.0, 128.0, 641.0, 0.0)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfw_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext=SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_df =sqlContext.createDataFrame(dfw_rdd, ['month','unique_carrier','origin_city_name','dest_city_name', 'dephrs', 'arrhrs','schduration','distance','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----------------+--------------+------+------+-----------+--------+-----+\n|month|unique_carrier|origin_city_name|dest_city_name|dephrs|arrhrs|schduration|distance|label|\n+-----+--------------+----------------+--------------+------+------+-----------+--------+-----+\n|  1.0|            UA|         Chicago|   Dallas/Fort|  12.0|  14.0|      150.0|   802.0|  0.0|\n|  1.0|            UA|          Denver|   Dallas/Fort|  10.0|  13.0|      116.0|   641.0|  0.0|\n|  1.0|            UA|     Dallas/Fort|       Chicago|  13.0|  16.0|      136.0|   802.0|  0.0|\n|  1.0|            UA|          Newark|   Dallas/Fort|   6.0|   9.0|      247.0|  1372.0|  0.0|\n|  1.0|            UA|     Dallas/Fort|        Denver|  16.0|  17.0|      128.0|   641.0|  0.0|\n|  1.0|            UA|          Denver|   Dallas/Fort|  15.0|  18.0|      112.0|   641.0|  0.0|\n|  1.0|            UA|     Dallas/Fort|    Washington|   7.0|  11.0|      171.0|  1172.0|  0.0|\n|  1.0|            UA|      Washington|   Dallas/Fort|  17.0|  19.0|      205.0|  1172.0|  0.0|\n|  1.0|            UA|             San|   Dallas/Fort|  10.0|  16.0|      210.0|  1464.0|  0.0|\n|  1.0|            UA|     Dallas/Fort|        Denver|   8.0|   9.0|      129.0|   641.0|  1.0|\n|  1.0|            UA|     Dallas/Fort|        Newark|  10.0|  14.0|      206.0|  1372.0|  0.0|\n|  1.0|            UA|         Chicago|   Dallas/Fort|  21.0|  23.0|      150.0|   802.0|  0.0|\n|  1.0|            UA|         Chicago|   Dallas/Fort|   6.0|   8.0|      157.0|   802.0|  0.0|\n|  1.0|            UA|     Dallas/Fort|        Newark|  17.0|  22.0|      207.0|  1372.0|  0.0|\n|  1.0|            UA|         Houston|   Dallas/Fort|  15.0|  17.0|       79.0|   224.0|  0.0|\n|  1.0|            UA|     Dallas/Fort|       Chicago|   9.0|  12.0|      144.0|   802.0|  0.0|\n|  1.0|            UA|     Dallas/Fort|        Denver|  18.0|  20.0|      130.0|   641.0|  0.0|\n|  1.0|            EV|     Dallas/Fort|       Houston|  12.0|  14.0|       71.0|   224.0|  0.0|\n|  1.0|            EV|         Chicago|   Dallas/Fort|  17.0|  20.0|      153.0|   802.0|  0.0|\n|  1.0|            EV|      Cincinnati|   Dallas/Fort|   9.0|  10.0|      158.0|   812.0|  0.0|\n+-----+--------------+----------------+--------------+------+------+-----------+--------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "input_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"unique_carrier\", outputCol=\"unique_carrierIndex\")\n",
    "model = stringIndexer.fit(input_df)\n",
    "indexed = model.transform(input_df)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"unique_carrierIndex\", outputCol=\"unique_carrierVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "\n",
    "stringIndexer1=StringIndexer(inputCol=\"origin_city_name\", outputCol=\"origin_city_nameIndex\")\n",
    "model = stringIndexer1.fit(encoded)\n",
    "indexed = model.transform(encoded)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"origin_city_nameIndex\", outputCol=\"origin_city_nameVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "\n",
    "stringIndexer2 = StringIndexer(inputCol=\"dest_city_name\", outputCol=\"dest_city_nameIndex\")\n",
    "model = stringIndexer2.fit(encoded)\n",
    "indexed = model.transform(encoded)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"dest_city_nameIndex\", outputCol=\"dest_city_nameIndexVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "\n",
    "transformeddf= encoded.select(\"month\", \"unique_carrierVec\",\"origin_city_nameVec\",\"dest_city_nameIndexVec\",\"dephrs\",\"arrhrs\",\"schduration\",\"distance\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+-------------------+----------------------+------+------+-----------+--------+-----+\n|month|unique_carrierVec|origin_city_nameVec|dest_city_nameIndexVec|dephrs|arrhrs|schduration|distance|label|\n+-----+-----------------+-------------------+----------------------+------+------+-----------+--------+-----+\n|  1.0|   (13,[6],[1.0])|    (127,[5],[1.0])|       (127,[0],[1.0])|  12.0|  14.0|      150.0|   802.0|  0.0|\n|  1.0|   (13,[6],[1.0])|    (127,[7],[1.0])|       (127,[0],[1.0])|  10.0|  13.0|      116.0|   641.0|  0.0|\n|  1.0|   (13,[6],[1.0])|    (127,[0],[1.0])|       (127,[5],[1.0])|  13.0|  16.0|      136.0|   802.0|  0.0|\n|  1.0|   (13,[6],[1.0])|   (127,[23],[1.0])|       (127,[0],[1.0])|   6.0|   9.0|      247.0|  1372.0|  0.0|\n|  1.0|   (13,[6],[1.0])|    (127,[0],[1.0])|       (127,[7],[1.0])|  16.0|  17.0|      128.0|   641.0|  0.0|\n|  1.0|   (13,[6],[1.0])|    (127,[7],[1.0])|       (127,[0],[1.0])|  15.0|  18.0|      112.0|   641.0|  0.0|\n|  1.0|   (13,[6],[1.0])|    (127,[0],[1.0])|       (127,[8],[1.0])|   7.0|  11.0|      171.0|  1172.0|  0.0|\n|  1.0|   (13,[6],[1.0])|    (127,[8],[1.0])|       (127,[0],[1.0])|  17.0|  19.0|      205.0|  1172.0|  0.0|\n|  1.0|   (13,[6],[1.0])|    (127,[1],[1.0])|       (127,[0],[1.0])|  10.0|  16.0|      210.0|  1464.0|  0.0|\n|  1.0|   (13,[6],[1.0])|    (127,[0],[1.0])|       (127,[7],[1.0])|   8.0|   9.0|      129.0|   641.0|  1.0|\n|  1.0|   (13,[6],[1.0])|    (127,[0],[1.0])|      (127,[25],[1.0])|  10.0|  14.0|      206.0|  1372.0|  0.0|\n|  1.0|   (13,[6],[1.0])|    (127,[5],[1.0])|       (127,[0],[1.0])|  21.0|  23.0|      150.0|   802.0|  0.0|\n|  1.0|   (13,[6],[1.0])|    (127,[5],[1.0])|       (127,[0],[1.0])|   6.0|   8.0|      157.0|   802.0|  0.0|\n|  1.0|   (13,[6],[1.0])|    (127,[0],[1.0])|      (127,[25],[1.0])|  17.0|  22.0|      207.0|  1372.0|  0.0|\n|  1.0|   (13,[6],[1.0])|    (127,[2],[1.0])|       (127,[0],[1.0])|  15.0|  17.0|       79.0|   224.0|  0.0|\n|  1.0|   (13,[6],[1.0])|    (127,[0],[1.0])|       (127,[5],[1.0])|   9.0|  12.0|      144.0|   802.0|  0.0|\n|  1.0|   (13,[6],[1.0])|    (127,[0],[1.0])|       (127,[7],[1.0])|  18.0|  20.0|      130.0|   641.0|  0.0|\n|  1.0|   (13,[2],[1.0])|    (127,[0],[1.0])|       (127,[2],[1.0])|  12.0|  14.0|       71.0|   224.0|  0.0|\n|  1.0|   (13,[2],[1.0])|    (127,[5],[1.0])|       (127,[0],[1.0])|  17.0|  20.0|      153.0|   802.0|  0.0|\n|  1.0|   (13,[2],[1.0])|   (127,[31],[1.0])|       (127,[0],[1.0])|   9.0|  10.0|      158.0|   812.0|  0.0|\n+-----+-----------------+-------------------+----------------------+------+------+-----------+--------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "transformeddf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(features=SparseVector(272, {0: 1.0, 7: 1.0, 19: 1.0, 141: 1.0, 268: 12.0, 269: 14.0, 270: 150.0, 271: 802.0}), label=0.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"month\", \"unique_carrierVec\", \"origin_city_nameVec\",\"dest_city_nameIndexVec\",\"dephrs\",\"arrhrs\",\"schduration\",\"distance\"],\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(transformeddf)\n",
    "print(output.select(\"features\", \"label\").first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finaldf= output.select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|            features|label|\n+--------------------+-----+\n|(272,[0,7,19,141,...|  0.0|\n|(272,[0,7,21,141,...|  0.0|\n|(272,[0,7,14,146,...|  0.0|\n|(272,[0,7,37,141,...|  0.0|\n|(272,[0,7,14,148,...|  0.0|\n|(272,[0,7,21,141,...|  0.0|\n|(272,[0,7,14,149,...|  0.0|\n|(272,[0,7,22,141,...|  0.0|\n|(272,[0,7,15,141,...|  0.0|\n|(272,[0,7,14,148,...|  1.0|\n|(272,[0,7,14,166,...|  0.0|\n|(272,[0,7,19,141,...|  0.0|\n|(272,[0,7,19,141,...|  0.0|\n|(272,[0,7,14,166,...|  0.0|\n|(272,[0,7,16,141,...|  0.0|\n|(272,[0,7,14,146,...|  0.0|\n|(272,[0,7,14,148,...|  0.0|\n|(272,[0,3,14,143,...|  0.0|\n|(272,[0,3,19,141,...|  0.0|\n|(272,[0,3,45,141,...|  0.0|\n+--------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "finaldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+--------------------+--------------------+\n|            features|label|prediction|       rawPrediction|         probability|\n+--------------------+-----+----------+--------------------+--------------------+\n|(272,[0,7,14,146,...|  0.0|       0.0|[1.27043438664910...|[0.78081709867488...|\n|(272,[0,7,15,141,...|  0.0|       0.0|[1.60640105276880...|[0.83291112019251...|\n|(272,[0,3,14,148,...|  0.0|       0.0|[1.23293374237094...|[0.77433163528321...|\n|(272,[0,3,66,141,...|  0.0|       0.0|[1.07893743862191...|[0.74629285080078...|\n|(272,[0,10,21,141...|  0.0|       0.0|[1.36279555244187...|[0.79621367258714...|\n|(272,[0,10,14,148...|  0.0|       0.0|[1.15858364114330...|[0.76107525980476...|\n|(272,[0,2,56,141,...|  0.0|       0.0|[0.99925317423100...|[0.73091171843632...|\n|(272,[0,2,14,230,...|  0.0|       0.0|[1.07398784857199...|[0.74535455434056...|\n|(272,[0,2,14,227,...|  0.0|       0.0|[0.72949095810343...|[0.67469355690360...|\n|(272,[0,3,14,143,...|  0.0|       0.0|[1.25115440831965...|[0.77749963080907...|\n|(272,[0,3,14,168,...|  0.0|       0.0|[1.33048184537023...|[0.79092032649133...|\n|(272,[0,3,14,172,...|  0.0|       0.0|[0.92511378882173...|[0.71608292858140...|\n|(272,[0,2,14,170,...|  0.0|       0.0|[1.34908277279058...|[0.79397963231958...|\n|(272,[0,2,129,141...|  0.0|       0.0|[1.16667544607398...|[0.76254356135475...|\n|(272,[0,2,14,229,...|  1.0|       0.0|[0.83176456529531...|[0.69672790854976...|\n|(272,[0,2,14,178,...|  0.0|       0.0|[1.19105894046468...|[0.76693040139000...|\n|(272,[0,2,60,141,...|  0.0|       0.0|[0.90946936944341...|[0.71289156705623...|\n|(272,[0,2,14,189,...|  0.0|       0.0|[1.53957704159663...|[0.82340323110382...|\n|(272,[0,2,14,154,...|  0.0|       0.0|[1.60426276791975...|[0.83261332281602...|\n|(272,[0,2,14,172,...|  0.0|       0.0|[1.01498533671053...|[0.73399465357541...|\n+--------------------+-----+----------+--------------------+--------------------+\nonly showing top 20 rows\n\n0.78372429407\n"
     ]
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import pyspark.sql.functions\n",
    "\n",
    "labeledPoint= (finaldf.select(col(\"label\"), col(\"features\"))\n",
    "  .map(lambda row: LabeledPoint(row.label, row.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, (272,[0,7,19,141,268,269,270,271],[1.0,1.0,1.0,1.0,12.0,14.0,150.0,802.0])),\n LabeledPoint(0.0, (272,[0,7,21,141,268,269,270,271],[1.0,1.0,1.0,1.0,10.0,13.0,116.0,641.0])),\n LabeledPoint(0.0, (272,[0,7,14,146,268,269,270,271],[1.0,1.0,1.0,1.0,13.0,16.0,136.0,802.0])),\n LabeledPoint(0.0, (272,[0,7,37,141,268,269,270,271],[1.0,1.0,1.0,1.0,6.0,9.0,247.0,1372.0])),\n LabeledPoint(0.0, (272,[0,7,14,148,268,269,270,271],[1.0,1.0,1.0,1.0,16.0,17.0,128.0,641.0]))]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeledPoint.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, (272,[0,7,19,141,268,269,270,271],[1.0,1.0,1.0,1.0,12.0,14.0,150.0,802.0])),\n LabeledPoint(0.0, (272,[0,7,21,141,268,269,270,271],[1.0,1.0,1.0,1.0,10.0,13.0,116.0,641.0])),\n LabeledPoint(0.0, (272,[0,7,14,146,268,269,270,271],[1.0,1.0,1.0,1.0,13.0,16.0,136.0,802.0])),\n LabeledPoint(0.0, (272,[0,7,21,141,268,269,270,271],[1.0,1.0,1.0,1.0,15.0,18.0,112.0,641.0])),\n LabeledPoint(0.0, (272,[0,7,14,166,268,269,270,271],[1.0,1.0,1.0,1.0,10.0,14.0,206.0,1372.0]))]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training, testing = labeledPoint.randomSplit([0.6, 0.4], seed=11)\n",
    "training.take(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0.0 Accuracy = 0.7833139845\nClass 1.0 Accuracy = 0.315789473684\nWeighted precision = 0.682004015475\nWeighted recall = 0.783274171283\nConfusion Matrix\n[[  1.74755000e+05   1.30000000e+01]\n [  4.83420000e+04   6.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression with labeled points\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "model = LogisticRegressionWithLBFGS.train(training, numClasses=2)\n",
    "predictionAndLabels = testing.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "labels = labeledPoint.map(lambda lp: lp.label).distinct().collect()\n",
    "for label in sorted(labels):\n",
    "    print(\"Class %s Accuracy = %s\" % (label, metrics.precision(label)))\n",
    "print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "print 'Confusion Matrix\\n', metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 2 with Spark 1.6",
   "name": "python2"
  },
  "language_info": {
   "pygments_lexer": "ipython2",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "name": "python",
   "file_extension": ".py",
   "version": "2.7.11",
   "codemirror_mode": {
    "version": 2,
    "name": "ipython"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
